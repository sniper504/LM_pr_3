# эта нейронка определяет рукописные цифры
from keras.datasets import mnist
import tensorflow
from keras import models, layers
from keras.utils import to_categorical
import matplotlib.pyplot as plt


(x_train,y_train),(x_test,y_test) = mnist.load_data() # возвращ 2 набора тренировочный и тестовый
x_train = x_train.reshape((60000, 28 * 28)).astype("float32")/255 # /255 для нормализации и эффетивного обучения , берем на вход 6000 изобр размерами 28*28 и приводим к типу флоат 32 для дальнейшей работы
x_test = x_test.reshape((10000, 28 * 28)).astype("float32")/255

print(y_train[0])
y_train = to_categorical(y_train)# то же делает что onehoyencoder
y_test = to_categorical(y_test)
print(y_train[0])

model = models.Sequential([ # последовательные слои в модели
    layers.Dense(512, activation='relu', input_shape = (784,)), # input layer
    layers.Dense(10, activation = 'softmax') # output layer; activation – путь активации,преобразует 10 чисел в вероятности;  dense – просто слой ; 10 – это 10 нейронов так как 10 классов
])

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy']) # loss это функция потерь; metrics это во время обучения должен быть учет какихто метрик

history = model.fit(x_train, y_train, epochs= 5, batch_size=128, validation_split= 0.1) 

# history.history['loss','val_loss','accuracy','val_accuracy']

plt.plot(history.history['loss'], label = "train_loss")
plt.plot(history.history['val_loss'], label = "val_loss")
plt.legend()
plt.show()